{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM3Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fLength    fWidth   fSize   fConc  fConc1     fAsym  fM3Long  fM3Trans  \\\n",
       "0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110   -8.2027   \n",
       "1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238   -9.9574   \n",
       "2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580  -45.2160   \n",
       "3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633   -7.1513   \n",
       "4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525   21.8393   \n",
       "\n",
       "    fAlpha     fDist class  \n",
       "0  40.0920   81.8828     g  \n",
       "1   6.3609  205.2610     g  \n",
       "2  76.9600  256.7880     g  \n",
       "3  10.4490  116.7370     g  \n",
       "4   4.6480  356.4620     g  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [\"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConc1\", \"fAsym\", \"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\", \"class\"]\n",
    "df=pd.read_csv(\"magic+gamma+telescope\\magic04.data\",names=feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class\"] = (df[\"class\"]==\"g\").astype(int)\n",
    "df[\"class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in feature_names[:-1]:\n",
    "#     plt.hist(df[df[\"class\"]==1][label], color='blue',label='gamma',alpha=0.7,density=True)\n",
    "#     plt.hist(df[df[\"class\"]==0][label], color='red',label='hadron',alpha=0.7,density=True)\n",
    "#     plt.title(label)\n",
    "#     plt.ylabel('prob')\n",
    "#     plt.xlabel(label)\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = np.split(df.sample(frac=1),[int(0.6*len(df)),int(0.8*len(df))])\n",
    "def scale_dataset(dataframe):\n",
    "    x = dataframe[dataframe.columns[:-1]].values\n",
    "    y = dataframe[dataframe.columns[-1]].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "\n",
    "    data = np.hstack((x, np.reshape(y,(-1,1))))\n",
    "\n",
    "    return data, x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, x_train, y_train = scale_dataset(train)\n",
    "dev, x_dev, y_dev = scale_dataset(dev)\n",
    "test, x_test, y_test = scale_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_model.fit(x_train,y_train)\n",
    "y_pred = knn_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 82.124080\n",
      "Accuracy on training set: 0.8212407991587802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.69      0.73      1364\n",
      "           1       0.84      0.90      0.87      2440\n",
      "\n",
      "    accuracy                           0.82      3804\n",
      "   macro avg       0.81      0.79      0.80      3804\n",
      "weighted avg       0.82      0.82      0.82      3804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: %f'%(np.mean(y_pred == y_test) * 100))\n",
    "print(\"Accuracy on training set:\", knn_model.score(x_test, y_test))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = GaussianNB()\n",
    "nb_model = nb_model.fit(x_train,y_train)\n",
    "y_pred = nb_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 72.213460\n",
      "Accuracy on training set: 0.7221345951629863\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.37      0.49      1364\n",
      "           1       0.72      0.92      0.81      2440\n",
      "\n",
      "    accuracy                           0.72      3804\n",
      "   macro avg       0.72      0.65      0.65      3804\n",
      "weighted avg       0.72      0.72      0.69      3804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: %f'%(np.mean(y_pred == y_test) * 100))\n",
    "print(\"Accuracy on training set:\", nb_model.score(x_test, y_test))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC()\n",
    "svm_model = svm_model.fit(x_train,y_train)\n",
    "y_pred = svm_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 86.303891\n",
      "Accuracy on training set: 0.8630389064143007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.69      0.78      1364\n",
      "           1       0.85      0.96      0.90      2440\n",
      "\n",
      "    accuracy                           0.86      3804\n",
      "   macro avg       0.88      0.83      0.84      3804\n",
      "weighted avg       0.87      0.86      0.86      3804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: %f'%(np.mean(y_pred == y_test) * 100))\n",
    "print(\"Accuracy on training set:\", svm_model.score(x_test, y_test))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "])\n",
    "nn_model.compile(optimizer=tf.keras.optimizers.Adam(0.001), \n",
    "                 loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 0.4392 - accuracy: 0.8066 - val_loss: 0.3892 - val_accuracy: 0.8191\n",
      "Epoch 2/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.8463 - val_loss: 0.3631 - val_accuracy: 0.8454\n",
      "Epoch 3/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8608 - val_loss: 0.3521 - val_accuracy: 0.8603\n",
      "Epoch 4/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.8654 - val_loss: 0.3508 - val_accuracy: 0.8555\n",
      "Epoch 5/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.8677 - val_loss: 0.3449 - val_accuracy: 0.8607\n",
      "Epoch 6/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8663 - val_loss: 0.3422 - val_accuracy: 0.8541\n",
      "Epoch 7/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8681 - val_loss: 0.3410 - val_accuracy: 0.8603\n",
      "Epoch 8/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3133 - accuracy: 0.8686 - val_loss: 0.3373 - val_accuracy: 0.8563\n",
      "Epoch 9/100\n",
      "286/286 [==============================] - 1s 2ms/step - loss: 0.3109 - accuracy: 0.8695 - val_loss: 0.3426 - val_accuracy: 0.8590\n",
      "Epoch 10/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8713 - val_loss: 0.3463 - val_accuracy: 0.8563\n",
      "Epoch 11/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3053 - accuracy: 0.8733 - val_loss: 0.3361 - val_accuracy: 0.8598\n",
      "Epoch 12/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3023 - accuracy: 0.8713 - val_loss: 0.3412 - val_accuracy: 0.8611\n",
      "Epoch 13/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.3007 - accuracy: 0.8729 - val_loss: 0.3329 - val_accuracy: 0.8611\n",
      "Epoch 14/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.8759 - val_loss: 0.3334 - val_accuracy: 0.8611\n",
      "Epoch 15/100\n",
      "286/286 [==============================] - 1s 2ms/step - loss: 0.2963 - accuracy: 0.8758 - val_loss: 0.3322 - val_accuracy: 0.8625\n",
      "Epoch 16/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8760 - val_loss: 0.3287 - val_accuracy: 0.8664\n",
      "Epoch 17/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2939 - accuracy: 0.8762 - val_loss: 0.3301 - val_accuracy: 0.8616\n",
      "Epoch 18/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.8770 - val_loss: 0.3281 - val_accuracy: 0.8633\n",
      "Epoch 19/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2917 - accuracy: 0.8759 - val_loss: 0.3291 - val_accuracy: 0.8651\n",
      "Epoch 20/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.8760 - val_loss: 0.3337 - val_accuracy: 0.8572\n",
      "Epoch 21/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8774 - val_loss: 0.3303 - val_accuracy: 0.8651\n",
      "Epoch 22/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2855 - accuracy: 0.8795 - val_loss: 0.3236 - val_accuracy: 0.8625\n",
      "Epoch 23/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2861 - accuracy: 0.8799 - val_loss: 0.3229 - val_accuracy: 0.8611\n",
      "Epoch 24/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.8814 - val_loss: 0.3251 - val_accuracy: 0.8647\n",
      "Epoch 25/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2829 - accuracy: 0.8807 - val_loss: 0.3239 - val_accuracy: 0.8607\n",
      "Epoch 26/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2826 - accuracy: 0.8826 - val_loss: 0.3224 - val_accuracy: 0.8620\n",
      "Epoch 27/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2810 - accuracy: 0.8817 - val_loss: 0.3238 - val_accuracy: 0.8590\n",
      "Epoch 28/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2786 - accuracy: 0.8817 - val_loss: 0.3296 - val_accuracy: 0.8625\n",
      "Epoch 29/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2783 - accuracy: 0.8806 - val_loss: 0.3231 - val_accuracy: 0.8668\n",
      "Epoch 30/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2783 - accuracy: 0.8831 - val_loss: 0.3218 - val_accuracy: 0.8651\n",
      "Epoch 31/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.8825 - val_loss: 0.3240 - val_accuracy: 0.8590\n",
      "Epoch 32/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8845 - val_loss: 0.3365 - val_accuracy: 0.8603\n",
      "Epoch 33/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8847 - val_loss: 0.3258 - val_accuracy: 0.8607\n",
      "Epoch 34/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2733 - accuracy: 0.8844 - val_loss: 0.3239 - val_accuracy: 0.8651\n",
      "Epoch 35/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.8848 - val_loss: 0.3245 - val_accuracy: 0.8616\n",
      "Epoch 36/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.8875 - val_loss: 0.3252 - val_accuracy: 0.8603\n",
      "Epoch 37/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.8879 - val_loss: 0.3293 - val_accuracy: 0.8642\n",
      "Epoch 38/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.8883 - val_loss: 0.3231 - val_accuracy: 0.8625\n",
      "Epoch 39/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2674 - accuracy: 0.8872 - val_loss: 0.3327 - val_accuracy: 0.8598\n",
      "Epoch 40/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2677 - accuracy: 0.8873 - val_loss: 0.3237 - val_accuracy: 0.8642\n",
      "Epoch 41/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.8889 - val_loss: 0.3236 - val_accuracy: 0.8607\n",
      "Epoch 42/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8889 - val_loss: 0.3268 - val_accuracy: 0.8625\n",
      "Epoch 43/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.8898 - val_loss: 0.3261 - val_accuracy: 0.8625\n",
      "Epoch 44/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.8901 - val_loss: 0.3235 - val_accuracy: 0.8664\n",
      "Epoch 45/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8905 - val_loss: 0.3249 - val_accuracy: 0.8625\n",
      "Epoch 46/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2601 - accuracy: 0.8901 - val_loss: 0.3353 - val_accuracy: 0.8642\n",
      "Epoch 47/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2599 - accuracy: 0.8905 - val_loss: 0.3309 - val_accuracy: 0.8616\n",
      "Epoch 48/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2595 - accuracy: 0.8924 - val_loss: 0.3227 - val_accuracy: 0.8616\n",
      "Epoch 49/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2600 - accuracy: 0.8902 - val_loss: 0.3189 - val_accuracy: 0.8629\n",
      "Epoch 50/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2593 - accuracy: 0.8928 - val_loss: 0.3359 - val_accuracy: 0.8607\n",
      "Epoch 51/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.8928 - val_loss: 0.3270 - val_accuracy: 0.8607\n",
      "Epoch 52/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.8926 - val_loss: 0.3337 - val_accuracy: 0.8651\n",
      "Epoch 53/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 0.8933 - val_loss: 0.3348 - val_accuracy: 0.8633\n",
      "Epoch 54/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2543 - accuracy: 0.8910 - val_loss: 0.3367 - val_accuracy: 0.8647\n",
      "Epoch 55/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2557 - accuracy: 0.8912 - val_loss: 0.3308 - val_accuracy: 0.8625\n",
      "Epoch 56/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.8940 - val_loss: 0.3370 - val_accuracy: 0.8611\n",
      "Epoch 57/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2526 - accuracy: 0.8952 - val_loss: 0.3303 - val_accuracy: 0.8611\n",
      "Epoch 58/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.8926 - val_loss: 0.3311 - val_accuracy: 0.8633\n",
      "Epoch 59/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2519 - accuracy: 0.8946 - val_loss: 0.3565 - val_accuracy: 0.8568\n",
      "Epoch 60/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.8948 - val_loss: 0.3390 - val_accuracy: 0.8629\n",
      "Epoch 61/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.8945 - val_loss: 0.3304 - val_accuracy: 0.8625\n",
      "Epoch 62/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.8954 - val_loss: 0.3362 - val_accuracy: 0.8629\n",
      "Epoch 63/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.8956 - val_loss: 0.3304 - val_accuracy: 0.8611\n",
      "Epoch 64/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.8983 - val_loss: 0.3301 - val_accuracy: 0.8598\n",
      "Epoch 65/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2471 - accuracy: 0.8943 - val_loss: 0.3362 - val_accuracy: 0.8625\n",
      "Epoch 66/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.8981 - val_loss: 0.3356 - val_accuracy: 0.8559\n",
      "Epoch 67/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.8966 - val_loss: 0.3357 - val_accuracy: 0.8664\n",
      "Epoch 68/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.8975 - val_loss: 0.3334 - val_accuracy: 0.8629\n",
      "Epoch 69/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.8990 - val_loss: 0.3384 - val_accuracy: 0.8629\n",
      "Epoch 70/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.8987 - val_loss: 0.3349 - val_accuracy: 0.8616\n",
      "Epoch 71/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2435 - accuracy: 0.8977 - val_loss: 0.3324 - val_accuracy: 0.8620\n",
      "Epoch 72/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2416 - accuracy: 0.9003 - val_loss: 0.3382 - val_accuracy: 0.8598\n",
      "Epoch 73/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2434 - accuracy: 0.8980 - val_loss: 0.3349 - val_accuracy: 0.8629\n",
      "Epoch 74/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2415 - accuracy: 0.8992 - val_loss: 0.3325 - val_accuracy: 0.8620\n",
      "Epoch 75/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2416 - accuracy: 0.9009 - val_loss: 0.3373 - val_accuracy: 0.8572\n",
      "Epoch 76/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.8991 - val_loss: 0.3362 - val_accuracy: 0.8598\n",
      "Epoch 77/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2409 - accuracy: 0.8987 - val_loss: 0.3380 - val_accuracy: 0.8629\n",
      "Epoch 78/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2381 - accuracy: 0.9025 - val_loss: 0.3384 - val_accuracy: 0.8598\n",
      "Epoch 79/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.9020 - val_loss: 0.3377 - val_accuracy: 0.8594\n",
      "Epoch 80/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2378 - accuracy: 0.9025 - val_loss: 0.3430 - val_accuracy: 0.8550\n",
      "Epoch 81/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2373 - accuracy: 0.9037 - val_loss: 0.3393 - val_accuracy: 0.8607\n",
      "Epoch 82/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.9045 - val_loss: 0.3401 - val_accuracy: 0.8590\n",
      "Epoch 83/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2354 - accuracy: 0.9046 - val_loss: 0.3394 - val_accuracy: 0.8625\n",
      "Epoch 84/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2352 - accuracy: 0.9038 - val_loss: 0.3422 - val_accuracy: 0.8563\n",
      "Epoch 85/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2346 - accuracy: 0.9014 - val_loss: 0.3367 - val_accuracy: 0.8607\n",
      "Epoch 86/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.9038 - val_loss: 0.3407 - val_accuracy: 0.8625\n",
      "Epoch 87/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.9029 - val_loss: 0.3488 - val_accuracy: 0.8550\n",
      "Epoch 88/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2325 - accuracy: 0.9035 - val_loss: 0.3402 - val_accuracy: 0.8581\n",
      "Epoch 89/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2321 - accuracy: 0.9034 - val_loss: 0.3395 - val_accuracy: 0.8598\n",
      "Epoch 90/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.9044 - val_loss: 0.3403 - val_accuracy: 0.8629\n",
      "Epoch 91/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2296 - accuracy: 0.9073 - val_loss: 0.3449 - val_accuracy: 0.8576\n",
      "Epoch 92/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2304 - accuracy: 0.9057 - val_loss: 0.3473 - val_accuracy: 0.8581\n",
      "Epoch 93/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2316 - accuracy: 0.9029 - val_loss: 0.3426 - val_accuracy: 0.8603\n",
      "Epoch 94/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2289 - accuracy: 0.9056 - val_loss: 0.3469 - val_accuracy: 0.8568\n",
      "Epoch 95/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9054 - val_loss: 0.3409 - val_accuracy: 0.8620\n",
      "Epoch 96/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2269 - accuracy: 0.9060 - val_loss: 0.3444 - val_accuracy: 0.8581\n",
      "Epoch 97/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2279 - accuracy: 0.9068 - val_loss: 0.3516 - val_accuracy: 0.8616\n",
      "Epoch 98/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2272 - accuracy: 0.9050 - val_loss: 0.3431 - val_accuracy: 0.8616\n",
      "Epoch 99/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.9059 - val_loss: 0.3434 - val_accuracy: 0.8642\n",
      "Epoch 100/100\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.9072 - val_loss: 0.3464 - val_accuracy: 0.8616\n"
     ]
    }
   ],
   "source": [
    "history = nn_model.fit(x_train,y_train,epochs=100,batch_size=32,validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
