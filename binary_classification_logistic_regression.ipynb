{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "\n",
    "my_ID = 400132290\n",
    "np.random.seed(my_ID)\n",
    "np.set_printoptions(precision=2)# reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (455, 30), X Type:<class 'numpy.ndarray'>)\n",
      "y Shape: (455,), y Type:<class 'numpy.ndarray'>)\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "x_data = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y_data = data.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=my_ID)\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "print(f\"X Shape: {x_train.shape}, X Type:{type(x_train)})\")\n",
    "#print(x_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**to do**\n",
    "- my version of Batch and SGD\n",
    "- compute missclassification rate\n",
    "- F1 ?\n",
    "- plot precision/recall PR curve and ROC curve\n",
    "- something about B???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations completed: [32]\n",
      "model parameters (batch): w: [-0.03], b:[[-0.48 -0.46 -0.47 -0.5  -0.09  0.56 -0.9  -1.    0.17  0.31 -1.11  0.12\n",
      "  -0.72 -0.87 -0.33  0.59  0.2  -0.24  0.49  0.48 -0.89 -1.28 -0.76 -0.87\n",
      "  -0.75  0.08 -1.08 -0.71 -0.94 -0.21]]\n",
      "Prediction on training set (batch):\n",
      "[0 1 0 1 1 1 1 1 1 1]\n",
      "Target values (batch)\n",
      "[0 1 0 1 1 1 1 1 1 1]\n",
      "number of iterations completed: 36\n",
      "model parameters (SGD): w: [-3.19], b:[[-1.85  0.52 -1.86 -1.75 -1.46  9.83 -6.44 -6.09  2.91 -2.29 -9.15  2.19\n",
      "  -2.9  -8.67 -3.8  -1.04  9.36 -4.68  3.9   4.85 -4.52 -9.59 -2.7  -5.87\n",
      "  -1.1   0.76 -8.23 -1.82 -6.26  0.27]]\n",
      "Prediction on training set (SGD):\n",
      "[0 1 0 1 1 1 1 1 1 1]\n",
      "Target values (SGD)\n",
      "[0 1 0 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# sklearn\n",
    "# batch\n",
    "log_reg =LogisticRegression(random_state=my_ID)\n",
    "log_reg.fit(x_train, y_train)\n",
    "print(f\"number of iterations completed: {log_reg.n_iter_}\")\n",
    "log_reg_b = log_reg.intercept_\n",
    "log_reg_w = log_reg.coef_\n",
    "print(f\"model parameters (batch): w: {log_reg_w}, b:{log_reg_b}\")\n",
    "\n",
    "predictions = log_reg.predict(x_test)\n",
    "print(f\"Prediction on training set (batch):\\n{predictions[:10]}\" )\n",
    "print(f\"Target values (batch)\\n{y_test[:10]}\")\n",
    "\n",
    "# SGD\n",
    "log_reg = SGDClassifier(random_state=my_ID)\n",
    "log_reg.fit(x_train, y_train)\n",
    "print(f\"number of iterations completed: {log_reg.n_iter_}\")\n",
    "log_reg_b = log_reg.intercept_\n",
    "log_reg_w = log_reg.coef_\n",
    "print(f\"model parameters (SGD): w: {log_reg_w}, b:{log_reg_b}\")\n",
    "\n",
    "predictions = log_reg.predict(x_test)\n",
    "print(f\"Prediction on training set (SGD):\\n{predictions[:10]}\" )\n",
    "# predictions_round = [0 if num < 0.5 else 1 for num in predictions]\n",
    "# print(f\"Prediction on training set rounded (SGD):\\n{predictions_round[:10]}\" )\n",
    "print(f\"Target values (SGD)\\n{y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "\n",
    "    g = 1/(1+np.exp(-z))\n",
    "   \n",
    "    return g\n",
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes cost\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "             \n",
    "    cost = cost / m\n",
    "    return cost\n",
    "\n",
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_db, dj_dw  \n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing\n",
    "\n",
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "    J_history = []\n",
    "   \n",
    "    ### START CODE HERE ### \n",
    "    # Loop over each example\n",
    "    for i in range(m):   \n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) +b)\n",
    "        \n",
    "        if f_wb_i >= 0.5:\n",
    "            p[i] = 1\n",
    "        else:\n",
    "            p[i] = 0\n",
    "        \n",
    "    ### END CODE HERE ### \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.5199155304053966   \n",
      "Iteration  100: Cost 0.10250037689934736   \n",
      "Iteration  200: Cost 0.08375939652798764   \n",
      "Iteration  300: Cost 0.0753040303767322   \n",
      "Iteration  400: Cost 0.07019446185843572   \n",
      "Iteration  500: Cost 0.06666817616987233   \n",
      "Iteration  600: Cost 0.06404283717825121   \n",
      "Iteration  700: Cost 0.061990257412144874   \n",
      "Iteration  800: Cost 0.06032955531009869   \n",
      "Iteration  900: Cost 0.05895127743229119   \n",
      "Output of predict: shape (455,)\n",
      "Train Accuracy: 98.901099\n",
      "\n",
      "updated parameters: w:[-0.6  -0.68 -0.58 -0.6  -0.23  0.17 -0.66 -0.75  0.06  0.47 -0.82  0.01\n",
      " -0.62 -0.65 -0.19  0.5   0.06 -0.14  0.41  0.47 -0.84 -1.05 -0.77 -0.78\n",
      " -0.79 -0.13 -0.79 -0.76 -0.69 -0.14], b:0.2254637406154479\n",
      "model parameters (batch): w: [[-1.85  0.52 -1.86 -1.75 -1.46  9.83 -6.44 -6.09  2.91 -2.29 -9.15  2.19\n",
      "  -2.9  -8.67 -3.8  -1.04  9.36 -4.68  3.9   4.85 -4.52 -9.59 -2.7  -5.87\n",
      "  -1.1   0.76 -8.23 -1.82 -6.26  0.27]], b:[-3.19]\n"
     ]
    }
   ],
   "source": [
    "w_tmp  = np.zeros_like(x_train[0])\n",
    "b_tmp  = 0.\n",
    "alph = 0.1\n",
    "iters = 1000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(x_train, y_train, w_tmp, b_tmp, alph, iters)\n",
    "\n",
    "p = predict(x_train, w_out, b_out)\n",
    "print(f'Output of predict: shape {p.shape}')\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))\n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")\n",
    "print(f\"model parameters (batch): w: {log_reg_w}, b:{log_reg_b}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
