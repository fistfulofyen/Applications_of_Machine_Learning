{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "from sklearn.metrics import PrecisionRecallDisplay, f1_score, mean_absolute_error, precision_recall_curve, \\\n",
    "    precision_score, recall_score\n",
    "my_ID = 400132290\n",
    "np.random.seed(my_ID)\n",
    "np.set_printoptions(precision=2)# reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (455, 30), X Type:<class 'numpy.ndarray'>)\n",
      "y Shape: (455,), y Type:<class 'numpy.ndarray'>)\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "x_data = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y_data = data.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=my_ID)\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "print(f\"X Shape: {x_train.shape}, X Type:{type(x_train)})\")\n",
    "#print(x_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**to do**\n",
    "- my version of Batch and SGD\n",
    "- compute missclassification rate\n",
    "- F1 ?\n",
    "- plot precision/recall PR curve and ROC curve\n",
    "- something about B???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without matrix multiplication\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "\n",
    "    g = 1/(1+np.exp(-z))\n",
    "   \n",
    "    return g\n",
    "\n",
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes cost\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "             \n",
    "    cost = cost / m\n",
    "    return cost\n",
    "\n",
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_db, dj_dw \n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing\n",
    "\n",
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "    J_history = []\n",
    "   \n",
    "    # Loop over each example\n",
    "    for i in range(m):   \n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) +b)\n",
    "        \n",
    "        if f_wb_i >= 0.5:\n",
    "            p[i] = 1\n",
    "        else:\n",
    "            p[i] = 0\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "def compute_gradient_logistic_SGD(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "\n",
    "    f_wb_i = sigmoid(np.dot(X,w) + b)          #(n,)(n,)=scalar\n",
    "    err_i  = f_wb_i  - y                       #scalar\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + err_i * X[j]      #scalar\n",
    "    dj_db = dj_db + err_i\n",
    "        \n",
    "    return dj_db, dj_dw  \n",
    "\n",
    "def gradient_descent_SGD(X, y, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    num_rows = X.shape[0]\n",
    "    for i in range(num_iters):\n",
    "        # generate random index and index the array to retrieve a roll\n",
    "        random_index = np.random.choice(num_rows)\n",
    "        \n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic_SGD(X[random_index], y[random_index], w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing\n",
    "\n",
    "# X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "# y_train = np.array([460, 232, 178])\n",
    "# b_init = 785.1811367994083\n",
    "# w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "#Compute and display gradient \n",
    "\n",
    "# tmp_dj_db, tmp_dj_dw = compute_gradient_logistic_SGD(X_train[i], y_train[i], w_init, b_init)\n",
    "# print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "# print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_tmp  = np.zeros_like(x_train[0])\n",
    "# b_tmp  = 0.\n",
    "# alph = 0.1\n",
    "# iters = 1000\n",
    "# w_out, b_out, _ = gradient_descent_SGD(x_train, y_train, w_tmp, b_tmp, alph, iters)\n",
    "\n",
    "# p_train = predict(x_train, w_out, b_out)\n",
    "# p_test = predict(x_test, w_out, b_out)\n",
    "# print(f'Output of predict: shape {p_test.shape}')\n",
    "# print('Train Accuracy: %f'%(np.mean(p_train == y_train) * 100))\n",
    "# print('Test Accuracy: %f'%(np.mean(p_test == y_test) * 100))\n",
    "# print(f\"model parameters (SGD): w: {w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_tmp  = np.zeros_like(x_train[0])\n",
    "# b_tmp  = 0.\n",
    "# alph = 0.1\n",
    "# iters = 1000\n",
    "\n",
    "# w_out, b_out, _ = gradient_descent(x_train, y_train, w_tmp, b_tmp, alph, iters)\n",
    "\n",
    "# p_train = predict(x_train, w_out, b_out)\n",
    "# p_test = predict(x_test, w_out, b_out)\n",
    "# print(f'Output of predict: shape {p_test.shape}')\n",
    "# print('Train Accuracy: %f'%(np.mean(p_train == y_train) * 100))\n",
    "# print('Test Accuracy: %f'%(np.mean(p_test == y_test) * 100))\n",
    "# print(f\"model parameters (batch): w: {w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log reg [ 0.23 -0.6  -0.68 -0.59 -0.6  -0.23  0.17 -0.66 -0.75  0.06  0.47 -0.82\n",
      "  0.01 -0.62 -0.65 -0.19  0.5   0.06 -0.14  0.41  0.47 -0.84 -1.05 -0.77\n",
      " -0.78 -0.79 -0.13 -0.79 -0.76 -0.69 -0.14] \n",
      "\n",
      "Train Accuracy: 98.901099\n",
      "Test Accuracy: 95.614035\n",
      "log reg SGD [ 0.51 -0.58 -0.79 -0.57 -0.61 -0.44  0.09 -0.76 -0.98  0.2   0.33 -1.11\n",
      " -0.07 -0.85 -0.85 -0.13  0.68  0.04 -0.17  0.41  0.43 -0.84 -1.15 -0.77\n",
      " -0.83 -0.82  0.07 -0.68 -0.94 -0.52 -0.07] \n",
      "\n",
      "Train Accuracy: 98.021978\n",
      "Test Accuracy: 95.614035\n"
     ]
    }
   ],
   "source": [
    "# matrix implementation\n",
    "def train_logistic_regression(x_train, y_train, a):\n",
    "    \"\"\"\n",
    "    Perform batch gradient descent to train the logistic regression model\n",
    "\n",
    "    :param x_train: the training input\n",
    "    :param y_train: the true values\n",
    "    :param tolerance: the threshold at which to stop training\n",
    "    :param lr: the learning rate\n",
    "    :return: the weights and bias for the model\n",
    "    \"\"\"\n",
    "    # initialize weights and bias to zero\n",
    "    weights = np.zeros(x_train.shape[1])\n",
    "    i = 0\n",
    "    while True:\n",
    "        # calculate predictions\n",
    "        preds = sigmoid(x_train @ weights)\n",
    "\n",
    "        # update the params\n",
    "        grad = (1 / x_train.shape[0]) * x_train.T @ (preds - y_train)\n",
    "\n",
    "        weights -= grad * a\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = y_train * np.logaddexp(0, -(x_train @ weights)) + (1 - y_train) * np.logaddexp(0, -(x_train @ weights))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i > 1000:\n",
    "            break\n",
    "\n",
    "    return weights\n",
    "\n",
    "def train_logistic_regression_SGD(x_train, y_train, a):\n",
    "    \"\"\"\n",
    "    Perform batch gradient descent to train the logistic regression model\n",
    "\n",
    "    :param x_train: the training input\n",
    "    :param y_train: the true values\n",
    "    :param tolerance: the threshold at which to stop training\n",
    "    :param lr: the learning rate\n",
    "    :return: the weights and bias for the model\n",
    "    \"\"\"\n",
    "    # initialize weights and bias to zero\n",
    "    weights = np.zeros(x_train.shape[1])\n",
    "    i = 0\n",
    "    num_rows = x_train.shape[0]\n",
    "    while True:\n",
    "        # randomly p\n",
    "        random_index = np.random.choice(num_rows)\n",
    "        x_train_SGD = x_train[random_index]\n",
    "        # calculate predictions\n",
    "        preds = sigmoid(x_train_SGD @ weights)\n",
    "\n",
    "        # update the params\n",
    "        grad = x_train_SGD.T * (preds - y_train[random_index])\n",
    "\n",
    "        weights -= grad * a\n",
    "\n",
    "        # calculate the loss\n",
    "        #loss = y_train * np.logaddexp(0, -(x_train @ weights)) + (1 - y_train) * np.logaddexp(0, -(x_train @ weights))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i > 1000:\n",
    "            break\n",
    "\n",
    "    return weights\n",
    "\n",
    "def predict_log_reg(x_test, weights, threshold=0.5):\n",
    "    preds = sigmoid(x_test @ weights)\n",
    "\n",
    "    # split classification at threshold\n",
    "    pred_class = [1 if i > threshold else 0 for i in preds]\n",
    "\n",
    "    return np.array(pred_class)\n",
    "\n",
    "\n",
    "# initialize data\n",
    "x_train_mat = np.c_[np.ones(x_train.shape[0]), x_train]\n",
    "x_test_mat = np.c_[np.ones(x_test.shape[0]), x_test]\n",
    "w_batch = train_logistic_regression(x_train_mat, y_train, 0.1)\n",
    "print(f\"log reg {w_batch}\",\"\\n\")\n",
    "\n",
    "p_train = predict_log_reg(x_train_mat, w_batch)\n",
    "p_test = predict_log_reg(x_test_mat, w_batch)\n",
    "print('Train Accuracy: %f'%(np.mean(p_train == y_train) * 100))\n",
    "print('Test Accuracy: %f'%(np.mean(p_test == y_test) * 100))\n",
    "\n",
    "w_batch = train_logistic_regression_SGD(x_train_mat, y_train, 0.1)\n",
    "print(f\"log reg SGD {w_batch}\",\"\\n\")\n",
    "\n",
    "p_train = predict_log_reg(x_train_mat, w_batch)\n",
    "p_test = predict_log_reg(x_test_mat, w_batch)\n",
    "print('Train Accuracy: %f'%(np.mean(p_train == y_train) * 100))\n",
    "print('Test Accuracy: %f'%(np.mean(p_test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations completed: [32]\n",
      "model parameters (batch): w: [[-0.48 -0.46 -0.47 -0.5  -0.09  0.56 -0.9  -1.    0.17  0.31 -1.11  0.12\n",
      "  -0.72 -0.87 -0.33  0.59  0.2  -0.24  0.49  0.48 -0.89 -1.28 -0.76 -0.87\n",
      "  -0.75  0.08 -1.08 -0.71 -0.94 -0.21]], b:[-0.03]\n",
      "The test error for the sklearn implementation is: 0.03508771929824561, the f1-score is: 0.9746835443037976\n",
      "number of iterations completed: 36\n",
      "model parameters (SGD): w: [[-1.85  0.52 -1.86 -1.75 -1.46  9.83 -6.44 -6.09  2.91 -2.29 -9.15  2.19\n",
      "  -2.9  -8.67 -3.8  -1.04  9.36 -4.68  3.9   4.85 -4.52 -9.59 -2.7  -5.87\n",
      "  -1.1   0.76 -8.23 -1.82 -6.26  0.27]], b:[-3.19]\n",
      "The test error for the sklearn implementation is: 0.03508771929824561, the f1-score is: 0.975\n"
     ]
    }
   ],
   "source": [
    "# sklearn\n",
    "# batch\n",
    "log_reg_batch =LogisticRegression(random_state=my_ID)\n",
    "log_reg_batch.fit(x_train, y_train)\n",
    "print(f\"number of iterations completed: {log_reg_batch.n_iter_}\")\n",
    "log_reg_batch_b = log_reg_batch.intercept_\n",
    "log_reg_batch_w = log_reg_batch.coef_\n",
    "print(f\"model parameters (batch): w: {log_reg_batch_w}, b:{log_reg_batch_b}\")\n",
    "\n",
    "predictions_batch = log_reg_batch.predict(x_test)\n",
    "#print(f\"Prediction on training set (batch):\\n{predictions_batch[:10]}\" )\n",
    "#print(f\"Target values (batch)\\n{y_test[:10]}\")\n",
    "\n",
    "best_misclass_rate = mean_absolute_error(y_test, predictions_batch)\n",
    "f1_s = f1_score(y_test, predictions_batch)\n",
    "print(f\"The test error for the sklearn implementation is: {best_misclass_rate}, the f1-score is: {f1_s}\")\n",
    "\n",
    "\n",
    "# SGD\n",
    "log_reg_SGD = SGDClassifier(random_state=my_ID)\n",
    "log_reg_SGD.fit(x_train, y_train)\n",
    "print(f\"number of iterations completed: {log_reg_SGD.n_iter_}\")\n",
    "log_reg_SGD_b = log_reg_SGD.intercept_\n",
    "log_reg_SGD_w = log_reg_SGD.coef_\n",
    "print(f\"model parameters (SGD): w: {log_reg_SGD_w}, b:{log_reg_SGD_b}\")\n",
    "\n",
    "predictions_SGD = log_reg_SGD.predict(x_test)\n",
    "#print(f\"Prediction on training set (SGD):\\n{predictions_SGD[:10]}\" )\n",
    "# predictions_round = [0 if num < 0.5 else 1 for num in predictions]\n",
    "# print(f\"Prediction on training set rounded (SGD):\\n{predictions_round[:10]}\" )\n",
    "#print(f\"Target values (SGD)\\n{y_test[:10]}\")\n",
    "\n",
    "best_misclass_rate = mean_absolute_error(y_test, predictions_SGD)\n",
    "f1_s = f1_score(y_test, predictions_SGD)\n",
    "print(f\"The test error for the sklearn implementation is: {best_misclass_rate}, the f1-score is: {f1_s}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
